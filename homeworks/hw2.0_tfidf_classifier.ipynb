{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import datetime\n",
    "\n",
    "from utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod(\n",
    "            (datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n",
    "              (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traintime = timer(None)\n",
    "train_time = timer(None)\n",
    "train = pd.read_csv('../data/toxic_comments/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../data/toxic_comments/test.csv').fillna(' ')\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "tr_ids = train[['id']]\n",
    "train[class_names] = train[class_names].astype(np.int8)\n",
    "target = train[class_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 17 minutes and 15.89 seconds.\n"
     ]
    }
   ],
   "source": [
    "train['comment_tokens'] = train['comment_text'].map(lambda x: tokenize(x))\n",
    "test['comment_tokens'] = test['comment_text'].map(lambda x: tokenize(x))\n",
    "\n",
    "train['comment_text'] = train['comment_tokens'].map(lambda x: ' '.join(x))\n",
    "test['comment_text'] = test['comment_tokens'].map(lambda x: ' '.join(x))\n",
    "\n",
    "train.drop([\"comment_tokens\"], axis=1, inplace=True)\n",
    "test.drop([\"comment_tokens\"], axis=1, inplace=True)\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "timer(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Part 1/2 of vectorizing ...\n",
      "\n",
      " Time taken: 0 hours 0 minutes and 56.44 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_time = timer(None)\n",
    "print(' Part 1/2 of vectorizing ...')\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "timer(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Part 2/2 of vectorizing ...\n",
      "\n",
      " Time taken: 0 hours 45 minutes and 23.35 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_time = timer(None)\n",
    "print(' Part 2/2 of vectorizing ...')\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "timer(train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union char and word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 1 hours 41 minutes and 32.83 seconds.\n"
     ]
    }
   ],
   "source": [
    "train_features = hstack([train_char_features, train_word_features]).tocsr()\n",
    "test_features = hstack([test_char_features, test_word_features]).tocsr()\n",
    "timer(traintime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_parameters = {\n",
    "    'C': [1.048113, 0.1930, 0.596362, 0.25595, 0.449843, 0.25595],\n",
    "    'tol': [0.1, 0.1, 0.046416, 0.0215443, 0.1, 0.01],\n",
    "    'solver': ['lbfgs', 'newton-cg', 'lbfgs', 'newton-cg', 'newton-cg', 'lbfgs'],\n",
    "    'fit_intercept': [True, True, True, True, True, True],\n",
    "    'penalty': ['l2', 'l2', 'l2', 'l2', 'l2', 'l2'],\n",
    "    'class_weight': [None, 'balanced', 'balanced', 'balanced', 'balanced', 'balanced'],\n",
    "}\n",
    "\n",
    "folds = 5\n",
    "scores = []\n",
    "scores_classes = np.zeros((len(class_names), folds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken: 0 hours 30 minutes and 54.47 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "submission_oof = train[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "\n",
    "idpred = tr_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Fold 01 class toxic AUC: 0.978427\n",
      "\n",
      " Fold 02 class toxic AUC: 0.977971\n",
      "\n",
      " Fold 03 class toxic AUC: 0.979633\n",
      "\n",
      " Fold 04 class toxic AUC: 0.978673\n",
      "\n",
      " Fold 05 class toxic AUC: 0.981027\n",
      "\n",
      " Average class toxic AUC:\t0.979146\n",
      " Out-of-fold class toxic AUC:\t0.979142\n",
      "\n",
      " Time taken: 0 hours 14 minutes and 27.11 seconds.\n",
      "\n",
      " Fold 01 class severe_toxic AUC: 0.987972\n",
      "\n",
      " Fold 02 class severe_toxic AUC: 0.988784\n",
      "\n",
      " Fold 03 class severe_toxic AUC: 0.990986\n",
      "\n",
      " Fold 04 class severe_toxic AUC: 0.987391\n",
      "\n",
      " Fold 05 class severe_toxic AUC: 0.989689\n",
      "\n",
      " Average class severe_toxic AUC:\t0.988965\n",
      " Out-of-fold class severe_toxic AUC:\t0.988953\n",
      "\n",
      " Time taken: 0 hours 22 minutes and 58.8 seconds.\n",
      "\n",
      " Fold 01 class obscene AUC: 0.991359\n",
      "\n",
      " Fold 02 class obscene AUC: 0.991048\n",
      "\n",
      " Fold 03 class obscene AUC: 0.990723\n",
      "\n",
      " Fold 04 class obscene AUC: 0.990488\n",
      "\n",
      " Fold 05 class obscene AUC: 0.991101\n",
      "\n",
      " Average class obscene AUC:\t0.990944\n",
      " Out-of-fold class obscene AUC:\t0.990943\n",
      "\n",
      " Time taken: 0 hours 10 minutes and 33.42 seconds.\n",
      "\n",
      " Fold 01 class threat AUC: 0.987219\n",
      "\n",
      " Fold 02 class threat AUC: 0.993277\n",
      "\n",
      " Fold 03 class threat AUC: 0.991606\n",
      "\n",
      " Fold 04 class threat AUC: 0.993415\n",
      "\n",
      " Fold 05 class threat AUC: 0.992120\n",
      "\n",
      " Average class threat AUC:\t0.991527\n",
      " Out-of-fold class threat AUC:\t0.991470\n",
      "\n",
      " Time taken: 0 hours 23 minutes and 6.53 seconds.\n",
      "\n",
      " Fold 01 class insult AUC: 0.982746\n",
      "\n",
      " Fold 02 class insult AUC: 0.983367\n",
      "\n",
      " Fold 03 class insult AUC: 0.984357\n",
      "\n",
      " Fold 04 class insult AUC: 0.983534\n",
      "\n",
      " Fold 05 class insult AUC: 0.983336\n",
      "\n",
      " Average class insult AUC:\t0.983468\n",
      " Out-of-fold class insult AUC:\t0.983460\n",
      "\n",
      " Time taken: 0 hours 23 minutes and 54.04 seconds.\n",
      "\n",
      " Fold 01 class identity_hate AUC: 0.984026\n",
      "\n",
      " Fold 02 class identity_hate AUC: 0.985899\n",
      "\n",
      " Fold 03 class identity_hate AUC: 0.983758\n",
      "\n",
      " Fold 04 class identity_hate AUC: 0.980711\n",
      "\n",
      " Fold 05 class identity_hate AUC: 0.982489\n",
      "\n",
      " Average class identity_hate AUC:\t0.983377\n",
      " Out-of-fold class identity_hate AUC:\t0.983353\n",
      "\n",
      " Time taken: 0 hours 10 minutes and 20.11 seconds.\n"
     ]
    }
   ],
   "source": [
    "traintime = timer(None)\n",
    "for j, (class_name) in enumerate(class_names):\n",
    "    #    train_target = train[class_name]\n",
    "\n",
    "    classifier = LogisticRegression(\n",
    "        C=all_parameters['C'][j],\n",
    "        max_iter=200,\n",
    "        tol=all_parameters['tol'][j],\n",
    "        solver=all_parameters['solver'][j],\n",
    "        fit_intercept=all_parameters['fit_intercept'][j],\n",
    "        penalty=all_parameters['penalty'][j],\n",
    "        dual=False,\n",
    "        class_weight=all_parameters['class_weight'][j],\n",
    "        verbose=0)\n",
    "\n",
    "    avreal = target[class_name]\n",
    "    lr_cv_sum = 0\n",
    "    lr_pred = []\n",
    "    lr_fpred = []\n",
    "    lr_avpred = np.zeros(train.shape[0])\n",
    "\n",
    "    train_time = timer(None)\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(train_features, target[class_name].values)):\n",
    "        X_train, X_val = train_features[train_index], train_features[val_index]\n",
    "        y_train, y_val = target.loc[train_index], target.loc[val_index]\n",
    "\n",
    "        classifier.fit(X_train, y_train[class_name])\n",
    "        scores_val = classifier.predict_proba(X_val)[:, 1]\n",
    "        lr_avpred[val_index] = scores_val\n",
    "        lr_y_pred = classifier.predict_proba(test_features)[:, 1]\n",
    "        scores_classes[j][i] = roc_auc_score(y_val[class_name], scores_val)\n",
    "        print('\\n Fold %02d class %s AUC: %.6f' % ((i + 1), class_name, scores_classes[j][i]))\n",
    "\n",
    "        if i > 0:\n",
    "            lr_fpred = lr_pred + lr_y_pred\n",
    "        else:\n",
    "            lr_fpred = lr_y_pred\n",
    "\n",
    "        lr_pred = lr_fpred\n",
    "\n",
    "    lr_cv_score = (lr_cv_sum / folds)\n",
    "    lr_oof_auc = roc_auc_score(avreal, lr_avpred)\n",
    "    print('\\n Average class %s AUC:\\t%.6f' % (class_name, np.mean(scores_classes[j])))\n",
    "    print(' Out-of-fold class %s AUC:\\t%.6f' % (class_name, lr_oof_auc))\n",
    "    timer(train_time)\n",
    "\n",
    "    submission[class_name] = lr_pred / folds\n",
    "    submission_oof['prediction_' + class_name] = lr_avpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Overall AUC:\t0.986238\n",
      "\n",
      " Time taken: 7 hours 46 minutes and 58.77 seconds.\n"
     ]
    }
   ],
   "source": [
    "print('\\n Overall AUC:\\t%.6f' % (np.mean(scores_classes)))\n",
    "submission.to_csv('submission_tuned_LR02_2_vectorizers.csv', index=False)\n",
    "submission_oof.to_csv('oof_tuned_LR02.csv', index=False)\n",
    "timer(traintime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fold 01 class toxic AUC: 0.977868\n",
    "\n",
    " Fold 02 class toxic AUC: 0.979100\n",
    "\n",
    " Fold 03 class toxic AUC: 0.977227\n",
    "\n",
    " Fold 04 class toxic AUC: 0.980188\n",
    "\n",
    " Fold 05 class toxic AUC: 0.978339\n",
    " \n",
    " Average class toxic AUC:\t0.978544\n",
    " Out-of-fold class toxic AUC:\t0.978549\n",
    "\n",
    " Time taken: 0 hours 13 minutes and 3.29 seconds.\n",
    "\n",
    " Fold 01 class severe_toxic AUC: 0.990015\n",
    "\n",
    " Fold 02 class severe_toxic AUC: 0.988798\n",
    "\n",
    " Fold 03 class severe_toxic AUC: 0.988556\n",
    "\n",
    " Fold 04 class severe_toxic AUC: 0.989866\n",
    "\n",
    " Fold 05 class severe_toxic AUC: 0.988006\n",
    "\n",
    " Average class severe_toxic AUC:\t0.989048\n",
    " Out-of-fold class severe_toxic AUC:\t0.989038\n",
    "\n",
    " Time taken: 0 hours 18 minutes and 51.72 seconds.\n",
    "\n",
    " Fold 01 class obscene AUC: 0.990285\n",
    " \n",
    " Fold 02 class obscene AUC: 0.990708\n",
    "\n",
    " Fold 03 class obscene AUC: 0.990812\n",
    "\n",
    " Fold 04 class obscene AUC: 0.991840\n",
    "\n",
    " Fold 05 class obscene AUC: 0.991806\n",
    "\n",
    " Average class obscene AUC:\t0.991090\n",
    " Out-of-fold class obscene AUC:\t0.991086\n",
    "\n",
    " Time taken: 0 hours 12 minutes and 14.55 seconds.\n",
    "\n",
    " Fold 01 class threat AUC: 0.991979\n",
    "\n",
    " Fold 02 class threat AUC: 0.992755\n",
    "\n",
    " Fold 03 class threat AUC: 0.992445\n",
    "\n",
    " Fold 04 class threat AUC: 0.985758\n",
    " \n",
    " Fold 05 class threat AUC: 0.992098\n",
    "\n",
    " Average class threat AUC:\t0.991007\n",
    " Out-of-fold class threat AUC:\t0.990981\n",
    "\n",
    " Time taken: 0 hours 19 minutes and 32.27 seconds.\n",
    "\n",
    " Fold 01 class insult AUC: 0.983164\n",
    "\n",
    " Fold 02 class insult AUC: 0.984527\n",
    " \n",
    " Fold 03 class insult AUC: 0.981855\n",
    " \n",
    " Fold 04 class insult AUC: 0.982395\n",
    " \n",
    " Fold 05 class insult AUC: 0.987513\n",
    " \n",
    " Average class insult AUC:\t0.9861212\n",
    " Out-of-fold class insult AUC:\t0.985211\n",
    " \n",
    " Fold 01 class identity_hate AUC: 0.992364\n",
    "\n",
    " Fold 02 class identity_hate AUC: 0.991254\n",
    " \n",
    " Fold 03 class identity_hate AUC: 0.993211\n",
    " \n",
    " Fold 04 class identity_hate AUC: 0.993521\n",
    " \n",
    " Fold 05 class identity_hate AUC: 0.990023\n",
    " \n",
    " Average class identity_hate AUC:\t0.992107\n",
    " Out-of-fold class identity_hate AUC:\t0.991901\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
